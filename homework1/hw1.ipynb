{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Task 1 (10 points)\n",
    "\n",
    "In the Ilias exercise session you found this PDF in, there is a file available called twit-\n",
    "ter.csv.bz2. Unpack this:\n",
    "bunzip2 twitter.csv.bz2\n",
    "Each line corresponds to one Tweet. The first column is an id, the second column\n",
    "is the Twitter handle (a user ID), the third column is the name of the user, the fourth\n",
    "column is the Tweet text (with special tokens [NEWLINE] and [TAB]).\n",
    "Implement a method index(filename) which takes the path to the file as an argument\n",
    "and puts all documents into a non-positional inverted index. You can assume that your\n",
    "computer’s memory is sufficient to store all postings lists. For the case that it is not,\n",
    "only index a subset of the documents, but it would be better if you think about how to\n",
    "make your program more efficient to make it fit (note that you do not need to store the\n",
    "document text as part of your index).\n",
    "The index should consist of a dictionary and postings lists. Each entry of the dictionary\n",
    "should contain three values: The (normalized) term, the size of the postings list, the\n",
    "pointer to the postings list. Your data structure should be prepared to be able to store\n",
    "the postings lists separately from the dictionary, therefore do not just put a List data\n",
    "structure as value into a HashMap/Tree or Python dictionary. Instead, put the postings\n",
    "lists into a data structure that you could store elsewhere (for instance, use a separate\n",
    "id-to-value mapping for that). It is your decision if and how you normalize tokens and\n",
    "terms. You can also decide to filter out Tweets if you think they are not relevant, to\n",
    "clean the data. Please describe your decisions in your submission.\n",
    "The postings list itself consists of postings which contain each a document id and a\n",
    "pointer to the next postings. For the dictionary, you can use hashing methods included\n",
    "in your programming language (like dictionary in Python or HashMap in Java) or tree\n",
    "structures as available in your programming language (for instance TreeMap in Java).\n",
    "For the postings lists, you can either implement the lists from scratch or use existing\n",
    "data structures (like lists in Python or LinkedList in Java).\n",
    "Then implement a method query(term), where the argument represents one term as\n",
    "a string. It should return the postings list for that term.\n",
    "Then, implement a method query(term1, term2), where you assume that both terms\n",
    "are connected with a logical and. Implement the intersection algorithm as discussed\n",
    "in the lecture for intersecting two postings lists. Do not access the lists array-style\n",
    "(for instance listname[5] where 5 is the position of the element you want to get).\n",
    "Use an iterator (in Python listiter = iter(listname); next(listiter) or in Java\n",
    "iterator.next()).\n",
    "You can choose the programming language. Comment your code! Submit all code in\n",
    "the same PDF as the other tasks (pretty printed). Please note, you won’t receive all\n",
    "points if the code is not commented properly.\n",
    "In addition, please query your index for the information need “show me tweets of\n",
    "people who talk about the side effects of malaria vaccines”. Provide us with your query\n",
    "and (a subset) of results. The results should be minimally represented by the Tweet-ID\n",
    "and optionally also the Tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        # stores size of postings list and pointer to list in a tuple using the normalized term as a key\n",
    "        self.dictionary = {}\n",
    "        # separate data structure which uses the pointer values of the dict as its keys to get the corresponding posting lists to an entry in the dict\n",
    "        self.postings_lists = {}\n",
    "        # the id counter is used to create new posting lists ids (in ascending order)\n",
    "        self.postings_list_id_counter = 0\n",
    "        self.stemmer = PorterStemmer() # used for stemming in normalization\n",
    "        self.dataset: pd.DataFrame = None # optional, used to retrieve tweet by id \n",
    "\n",
    "    def normalize_term(self, term: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize the term by converting it to lowercase, removing any non-alphanumeric characters, and stemming.\n",
    "        \"\"\"\n",
    "        term = re.sub(r\"\\W+\", \"\", term.lower())\n",
    "        # we decided to stem, as it is fast and we do not mind the risk of overstemming, as the doc size is small therefore wrong results can fastely be identified by the user.\n",
    "        stemmed_term = self.stemmer.stem(term)\n",
    "        return stemmed_term\n",
    "    \n",
    "    def get_tweet_texts(self, tweet_ids: List[str]) -> List[str]:\n",
    "        '''\n",
    "        Get the text content of tweets given their IDs.\n",
    "        '''\n",
    "        \n",
    "        # filter the DataFrame to only include rows with tweet_id in tweet_ids\n",
    "        filtered_df = self.dataset[self.dataset['tweet_id'].isin(tweet_ids)]\n",
    "\n",
    "        # return the text column of the filtered DataFrame\n",
    "        return filtered_df['text'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "    def index(self, filename: str):\n",
    "        \"\"\"\n",
    "        Index the documents in the given file.\n",
    "        \"\"\"\n",
    "        # use quoting = 3 to ignore separators in quotes\n",
    "        self.dataset = pd.read_csv(filename, sep='\\t', header=None, names=['date', 'tweet_id', 'handle', 'name', 'text'], quoting=3)\n",
    "        # drop content duplicates if everything except tweet id is identical\n",
    "        # why? compresses size and removes redundancy, if texts like parols are written multiple times they will be included, as the date will be different each time\n",
    "        self.dataset = self.dataset.drop_duplicates(subset=['date', 'handle', 'name', 'text'])\n",
    "        # sort lines ascending by tweet id, so the postings are inserted in a sorted way automatically\n",
    "        self.dataset = self.dataset.sort_values(by='tweet_id').reset_index(drop=True)\n",
    "        # one line per tweet\n",
    "        for _, row in self.dataset.iterrows():\n",
    "            tweet_id = int(row['tweet_id']) # extract tweet id\n",
    "            tweet_text = str(row['text']) # extract tweet string\n",
    "            terms = tweet_text.split() # split on any whitespace char\n",
    "            unique_terms = set()\n",
    "            for term in terms:\n",
    "                # normalize for better query results and less redundant terms\n",
    "                normalized_term = self.normalize_term(term)\n",
    "                if normalized_term and normalized_term not in unique_terms:\n",
    "                    unique_terms.add(normalized_term)\n",
    "                    if normalized_term not in self.dictionary:\n",
    "                        postings_list_id = self.postings_list_id_counter\n",
    "                        # create posting list entry for new term\n",
    "                        self.postings_lists[postings_list_id] = []\n",
    "                        # store pointer to posting list in dict\n",
    "                        self.dictionary[normalized_term] = (0, postings_list_id)\n",
    "                        self.postings_list_id_counter += 1\n",
    "                        \n",
    "                    # get posting list of normalized term\n",
    "                    size, postings_list_id = self.dictionary[normalized_term]\n",
    "                    postings_list = self.postings_lists[postings_list_id]\n",
    "                    # if no postings in list or last posting list entry does not match id, append the new tweet and let next point to none\n",
    "                    if not postings_list or postings_list[-1][0] != tweet_id:\n",
    "                        postings_list.append((tweet_id, None))\n",
    "                        # update postings list size for term\n",
    "                        self.dictionary[normalized_term] = (\n",
    "                            size + 1,\n",
    "                            postings_list_id,\n",
    "                        )\n",
    "                        if len(postings_list) > 1:\n",
    "                            # update old end-of-postings pointer from None to new entry\n",
    "                            postings_list[-2] = (\n",
    "                                postings_list[-2][0],\n",
    "                                len(postings_list) - 1,\n",
    "                            )\n",
    "\n",
    "    def query_single_term(self, term: str) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Query the index for a single term and return the postings list.\n",
    "        \"\"\"\n",
    "        # normalize query term before checking entries in dict\n",
    "        normalized_term = self.normalize_term(term)\n",
    "        if normalized_term in self.dictionary:\n",
    "            size, postings_list_id = self.dictionary[normalized_term]\n",
    "            return self.postings_lists[postings_list_id]\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex(InvertedIndex):\n",
    "    def intersect_postings_lists(\n",
    "        self,\n",
    "        postings_list1: List[Tuple[int, int]],\n",
    "        postings_list2: List[Tuple[int, int]],\n",
    "    ) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Intersect two postings lists and return the common document IDs.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        iter1 = iter(postings_list1)\n",
    "        iter2 = iter(postings_list2)\n",
    "        posting1 = next(iter1, None)\n",
    "        posting2 = next(iter2, None)\n",
    "        # implementation of two lists as shown in the lecture, precondition: postings must be sorted in ascending order (was ensured in index method)\n",
    "        while posting1 is not None and posting2 is not None:\n",
    "            doc_id1, next_posting1 = posting1\n",
    "            doc_id2, next_posting2 = posting2\n",
    "            if doc_id1 == doc_id2:\n",
    "                if len(result) > 0:\n",
    "                    result[-1] = (result[-1][0], len(result))\n",
    "                result.append((doc_id1, None))\n",
    "                posting1 = next(iter1, None) if next_posting1 is not None else None\n",
    "                posting2 = next(iter2, None) if next_posting2 is not None else None\n",
    "            elif doc_id1 < doc_id2:\n",
    "                posting1 = next(iter1, None) if next_posting1 is not None else None\n",
    "            else:\n",
    "                posting2 = next(iter2, None) if next_posting2 is not None else None\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def query(self, *terms: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Query the index for any number of AND combined terms and return the document IDs.\n",
    "        \"\"\"\n",
    "        if not terms:\n",
    "            return []\n",
    "    \n",
    "        # get postings list for the first term\n",
    "        postings_list = self.query_single_term(terms[0])\n",
    "    \n",
    "        # intersect postings lists for the remaining terms\n",
    "        for term in terms[1:]:\n",
    "            postings_list2 = self.query_single_term(term)\n",
    "            postings_list = self.intersect_postings_lists(postings_list, postings_list2)\n",
    "    \n",
    "        # return tweet IDs, without next pointers\n",
    "        return [doc_id for doc_id, _ in postings_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for tweets file\n",
    "index = InvertedIndex()\n",
    "index.index(\"tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[968853898185314306, 968853932960251904, 968855540985204738]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Steroid-based compounds against Malaria: highly effective, synergistic to artemisinin, no resistance, no side effects, up-scaling possible. #malaria  https://t.co/minlnwx1f7',\n",
       " 'Steroid-based compounds against Malaria: highly effective, synergistic to artemisinin, no resistance, no side... https://t.co/YDAIkL7jla',\n",
       " 'Steroid-based compounds against Malaria: highly effective, synergistic to artemisinin, no resistance, no side effects, up-scaling possible. #malaria  https://t.co/mm8ne1EGVS @jlugiessen @GICAfrica @GSK https://t.co/UQoDej13Uu']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# “show me tweets of people who talk about the side effects of malaria vaccines”\n",
    "resp = index.query(\"malaria\", \"side\", \"effects\")\n",
    "\n",
    "print(resp)\n",
    "\n",
    "index.get_tweet_texts(resp)\n",
    "\n",
    "\n",
    "# COMMENT: seems to be what we searched for, but if we include vaccine in the request we will not find these posts as they do not mention vaccines directly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
