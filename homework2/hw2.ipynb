{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Task 2 (Subtask 2) (10 points)\n",
    "\n",
    "This task is more suitable if you do have an implementation you can build on top of\n",
    "from assignment 1. \\\n",
    "Choose bigram index or permuterm index and **implement one of these approaches**.\n",
    "Define **4 queries which contain two search terms connected with AND**. \\\n",
    "At least one of the two search terms should contain a wildcard.\n",
    "\n",
    "(1) One of the four queries should have the wildcard on the left \\\n",
    "(2) one should have the wildcard on the right \\\n",
    "(3) one should have a wildcard between other characters \\\n",
    "(4) one should have one wildcard on the left and one on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import re # only used for normalization\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self):\n",
    "        # stores size of postings list and pointer to list in a tuple using the normalized term as a key\n",
    "        self.dictionary = {}\n",
    "        # separate data structure which uses the pointer values of the dict as its keys to get the corresponding posting lists to an entry in the dict\n",
    "        self.postings_lists = {}\n",
    "        # the id counter is used to create new posting lists ids (in ascending order)\n",
    "        self.postings_list_id_counter = 0\n",
    "        self.dataset: pd.DataFrame = None # optional, used to retrieve tweet by id \n",
    "        self.permuterm_index = {} # used for wildcard queries\n",
    "\n",
    "    def normalize_term(self, term: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize the term by converting it to lowercase, removing any non-alphanumeric characters, and stemming.\n",
    "        \"\"\"\n",
    "        term = re.sub(r\"\\W+\", \"\", term.lower())\n",
    "        return term\n",
    "    \n",
    "    def get_tweet_texts(self, tweet_ids: List[str]) -> List[str]:\n",
    "        '''\n",
    "        Get the text content of tweets given their IDs.\n",
    "        '''\n",
    "        \n",
    "        # filter the DataFrame to only include rows with tweet_id in tweet_ids\n",
    "        filtered_df = self.dataset[self.dataset['tweet_id'].isin(tweet_ids)]\n",
    "\n",
    "        # return the text column of the filtered DataFrame\n",
    "        return filtered_df['text'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "    def index(self, filename: str, permuterm_index: bool = True):\n",
    "        \"\"\"\n",
    "        Index the documents in the given file.\n",
    "        \"\"\"\n",
    "        # use quoting = 3 to ignore separators in quotes\n",
    "        self.dataset = pd.read_csv(filename, sep='\\t', header=None, names=['date', 'tweet_id', 'handle', 'name', 'text'], quoting=3)\n",
    "        # drop content duplicates if everything except tweet id is identical\n",
    "        # why? compresses size and removes redundancy, if texts like parols are written multiple times they will be included, as the date will be different each time\n",
    "        self.dataset = self.dataset.drop_duplicates(subset=['date', 'handle', 'name', 'text'])\n",
    "        # sort lines ascending by tweet id, so the postings are inserted in a sorted way automatically\n",
    "        self.dataset = self.dataset.sort_values(by='tweet_id').reset_index(drop=True)\n",
    "        # one line per tweet\n",
    "        for _, row in self.dataset.iterrows():\n",
    "            tweet_id = int(row['tweet_id']) # extract tweet id\n",
    "            tweet_text = str(row['text']) # extract tweet string\n",
    "            terms = tweet_text.split() # split on any whitespace char\n",
    "            unique_terms = set()\n",
    "            for term in terms:\n",
    "                # normalize for better query results and less redundant terms\n",
    "                normalized_term = self.normalize_term(term)\n",
    "                if normalized_term and normalized_term not in unique_terms:\n",
    "                    unique_terms.add(normalized_term)\n",
    "                    if normalized_term not in self.dictionary:\n",
    "                        postings_list_id = self.postings_list_id_counter\n",
    "                        # create posting list entry for new term\n",
    "                        self.postings_lists[postings_list_id] = []\n",
    "                        # store pointer to posting list in dict\n",
    "                        self.dictionary[normalized_term] = (0, postings_list_id)\n",
    "                        self.postings_list_id_counter += 1\n",
    "                        \n",
    "                    # get posting list of normalized term\n",
    "                    size, postings_list_id = self.dictionary[normalized_term]\n",
    "                    postings_list = self.postings_lists[postings_list_id]\n",
    "                    # if no postings in list or last posting list entry does not match id, append the new tweet and let next point to none\n",
    "                    if not postings_list or postings_list[-1][0] != tweet_id:\n",
    "                        postings_list.append((tweet_id, None))\n",
    "                        # update postings list size for term\n",
    "                        self.dictionary[normalized_term] = (\n",
    "                            size + 1,\n",
    "                            postings_list_id,\n",
    "                        )\n",
    "                        if len(postings_list) > 1:\n",
    "                            # update old end-of-postings pointer from None to new entry\n",
    "                            postings_list[-2] = (\n",
    "                                postings_list[-2][0],\n",
    "                                len(postings_list) - 1,\n",
    "                            )\n",
    "        \n",
    "        # add permuterm index if requested\n",
    "        if permuterm_index:\n",
    "            self.build_permuterm_index()\n",
    "\n",
    "    def query_single_term(self, term: str) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Query the index for a single term and return the postings list.\n",
    "        \"\"\"\n",
    "        # normalize query term before checking entries in dict\n",
    "        normalized_term = self.normalize_term(term)\n",
    "        if normalized_term in self.dictionary:\n",
    "            size, postings_list_id = self.dictionary[normalized_term]\n",
    "            return self.postings_lists[postings_list_id]\n",
    "        return []\n",
    "    \n",
    "    def build_permuterm_index(self):\n",
    "        \"\"\"Build a permuterm index for a given set of terms.\"\"\"\n",
    "        for term in self.dictionary.keys():\n",
    "            # add the special symbol to mark the end and then generate permutations (as we normalize, we can assume that this symbol was not in the term)\n",
    "            rotated_term = term + '$'\n",
    "            for i in range(len(rotated_term)):\n",
    "                # rotate the term\n",
    "                rotated_term = rotated_term[1:] + rotated_term[0]\n",
    "                # insert into the index\n",
    "                if rotated_term not in self.permuterm_index:\n",
    "                    self.permuterm_index[rotated_term] = set()\n",
    "                self.permuterm_index[rotated_term].add(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index(Index):\n",
    "    def intersect_postings_lists(\n",
    "        self,\n",
    "        postings_list1: List[Tuple[int, int]],\n",
    "        postings_list2: List[Tuple[int, int]],\n",
    "    ) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Intersect two postings lists and return the common document IDs.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        iter1 = iter(postings_list1)\n",
    "        iter2 = iter(postings_list2)\n",
    "        posting1 = next(iter1, None)\n",
    "        posting2 = next(iter2, None)\n",
    "        # implementation of two lists as shown in the lecture, precondition: postings must be sorted in ascending order (was ensured in index method)\n",
    "        while posting1 is not None and posting2 is not None:\n",
    "            doc_id1, next_posting1 = posting1\n",
    "            doc_id2, next_posting2 = posting2\n",
    "            if doc_id1 == doc_id2:\n",
    "                if len(result) > 0:\n",
    "                    result[-1] = (result[-1][0], len(result))\n",
    "                result.append((doc_id1, None))\n",
    "                posting1 = next(iter1, None) if next_posting1 is not None else None\n",
    "                posting2 = next(iter2, None) if next_posting2 is not None else None\n",
    "            elif doc_id1 < doc_id2:\n",
    "                posting1 = next(iter1, None) if next_posting1 is not None else None\n",
    "            else:\n",
    "                posting2 = next(iter2, None) if next_posting2 is not None else None\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def merge_postings_lists(\n",
    "        self,\n",
    "        postings_list1: List[Tuple[int, int]],\n",
    "        postings_list2: List[Tuple[int, int]],\n",
    "    ) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Merge two postings lists and return the common document IDs. (Used for OR queries of wildcard terms)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        iter1 = iter(postings_list1)\n",
    "        iter2 = iter(postings_list2)\n",
    "        posting1 = next(iter1, None)\n",
    "        posting2 = next(iter2, None)\n",
    "\n",
    "        while posting1 is not None and posting2 is not None:\n",
    "            doc_id1, _ = posting1\n",
    "            doc_id2, _ = posting2\n",
    "\n",
    "            # append one if the same and advance both\n",
    "            if doc_id1 == doc_id2:\n",
    "                result.append(posting1)\n",
    "                posting1 = next(iter1, None)\n",
    "                posting2 = next(iter2, None)\n",
    "            # append the smaller one and advance it\n",
    "            elif doc_id1 < doc_id2:\n",
    "                result.append(posting1)\n",
    "                posting1 = next(iter1, None)\n",
    "            # advance the other pointer if doc_id2 < doc_id1\n",
    "            else:\n",
    "                result.append(posting2)\n",
    "                posting2 = next(iter2, None)\n",
    "\n",
    "        # ad remaining items from either list if one list is exhausted before the other\n",
    "        while posting1 is not None:\n",
    "            result.append(posting1)\n",
    "            posting1 = next(iter1, None)\n",
    "\n",
    "        while posting2 is not None:\n",
    "            result.append(posting2)\n",
    "            posting2 = next(iter2, None)\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def query(self, *terms: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Query the index for any number of AND combined terms and return the document IDs.\n",
    "        Wildcard queries are supported.\n",
    "        \"\"\"\n",
    "        if not terms:\n",
    "            return []\n",
    "        \n",
    "        postings_lists_and = []\n",
    "        \n",
    "        for term in terms:\n",
    "            or_terms = set()\n",
    "            if '*' not in term:\n",
    "                # No wildcard, direct search\n",
    "                postings_lists_and.append(self.query_single_term(term))\n",
    "            else:\n",
    "                # Process the wildcard query\n",
    "                left_wildcard = term.startswith('*')\n",
    "                right_wildcard = term.endswith('*')\n",
    "                query_parts = term.strip('*').split('*')\n",
    "\n",
    "                if left_wildcard and right_wildcard:\n",
    "                    # Wildcard on both sides, take the middle part without $ (p.4/87)\n",
    "                    search_term = query_parts[0]\n",
    "                elif left_wildcard:\n",
    "                    # Wildcard on the left\n",
    "                    search_term = query_parts[0] + '$'\n",
    "                elif right_wildcard:\n",
    "                    # Wildcard on the right\n",
    "                    search_term = '$' + query_parts[0]\n",
    "                else:\n",
    "                    # Wildcard in the middle\n",
    "                    search_term = query_parts[1] + '$' + query_parts[0]\n",
    "\n",
    "                for key in self.permuterm_index:\n",
    "                    if key.startswith(search_term):\n",
    "                        for t in self.permuterm_index[key]:\n",
    "                            or_terms.add(t)\n",
    "                \n",
    "                if not or_terms: # no matching terms found, AND query will be empty, so you can already ret empty list\n",
    "                    return []\n",
    "                \n",
    "                or_terms = list(or_terms) # back to list for iterating\n",
    "\n",
    "                # get posting list for all wildcard matches with OR query\n",
    "                postings_list = self.query_single_term(or_terms[0])\n",
    "                for term in or_terms[1:]:\n",
    "                    postings_list2 = self.query_single_term(term)\n",
    "                    postings_list = self.merge_postings_lists(postings_list, postings_list2)\n",
    "                \n",
    "                # append OR query result to AND query list, to intersect results of wildcard term with other posting lists\n",
    "                postings_lists_and.append(postings_list)\n",
    "\n",
    "        postings_list = postings_lists_and[0]\n",
    "        # AND query\n",
    "        for l in postings_lists_and[1:]:\n",
    "            postings_list = self.intersect_postings_lists(postings_list, l)\n",
    "\n",
    "        return [doc_id for doc_id, _ in postings_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for tweets file\n",
    "index = Index()\n",
    "index.index(\"tweets.csv\", permuterm_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[968853898185314306, 968855540985204738]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Steroid-based compounds against Malaria: highly effective, synergistic to artemisinin, no resistance, no side effects, up-scaling possible. #malaria  https://t.co/minlnwx1f7',\n",
       " 'Steroid-based compounds against Malaria: highly effective, synergistic to artemisinin, no resistance, no side effects, up-scaling possible. #malaria  https://t.co/mm8ne1EGVS @jlugiessen @GICAfrica @GSK https://t.co/UQoDej13Uu']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OLD TASK (still works)\n",
    "# “show me tweets of people who talk about the side effects of malaria vaccines”\n",
    "resp = index.query(\"malaria\", \"side\", \"effects\")\n",
    "\n",
    "print(resp)\n",
    "\n",
    "index.get_tweet_texts(resp)\n",
    "\n",
    "\n",
    "# COMMENT: seems to be what we searched for, but if we include vaccine in the request we will not find these posts as they do not mention vaccines directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae0df862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[958849246438010881]\n",
      "['@adamwiththebmw @Suzbehnan @K_Janeski This!!! I hate it when people say “White people”, because the difference between American and European behavior is huge. Like, please don’t throw us under the same bus as them, we have almost nothing in common, except for our skin color.']\n",
      "[958849246438010881]\n",
      "['@adamwiththebmw @Suzbehnan @K_Janeski This!!! I hate it when people say “White people”, because the difference between American and European behavior is huge. Like, please don’t throw us under the same bus as them, we have almost nothing in common, except for our skin color.']\n",
      "[958849246438010881]\n",
      "['@adamwiththebmw @Suzbehnan @K_Janeski This!!! I hate it when people say “White people”, because the difference between American and European behavior is huge. Like, please don’t throw us under the same bus as them, we have almost nothing in common, except for our skin color.']\n",
      "[959008534120759296]\n",
      "['Cancer “vaccine” makes tumours vanish https://t.co/gzdlUaP2sT #science via @CosmosMagazine']\n"
     ]
    }
   ],
   "source": [
    "# SUBTASK 2 queries\n",
    "\n",
    "# test wildcard queries (subtask 2)\n",
    "\n",
    "# (1) One of the four queries should have the wildcard on the left\n",
    "resp = index.query(\"*ple\", \"white\")\n",
    "print(resp)\n",
    "print(index.get_tweet_texts(resp))\n",
    "# (2) one should have the wildcard on the right\n",
    "resp = index.query(\"pe*\", \"white\")\n",
    "print(resp)\n",
    "print(index.get_tweet_texts(resp))\n",
    "# (3) one should have a wildcard between other characters\n",
    "resp = index.query(\"adam*bmw\", \"and\")\n",
    "print(resp)\n",
    "print(index.get_tweet_texts(resp))\n",
    "# (4) one should have one wildcard on the left and one on the right.\n",
    "resp = index.query(\"*accin*\", \"canc*\", \"tumours\")\n",
    "print(resp)\n",
    "print(index.get_tweet_texts(resp))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
